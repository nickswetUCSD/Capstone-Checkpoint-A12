{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/RobustBench/robustbench.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edcc44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "import torch.utils.data as torch_data \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from robustbench.data import load_cifar10c\n",
    "import copy\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f1656",
   "metadata": {},
   "source": [
    "# Test Time Adaptation with Hard Pseudolabeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ca2d5",
   "metadata": {},
   "source": [
    "First initialize neural network architecture, we start with the `BasicBlock` class, which consists of two convolutional layers and a shortcut connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24137838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f7805",
   "metadata": {},
   "source": [
    "The `ResNet` class stacks these blocks in layers to create a deep neural network, which will be used to train our source classifier on the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a93c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, feature_maps=False):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out1 = self.layer1(out)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out = self.layer4(out3)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out4 = out.view(out.size(0), -1)\n",
    "        out = self.linear(out4)\n",
    "\n",
    "        if feature_maps:\n",
    "            return out, [out1, out2, out3, out4]\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c6541",
   "metadata": {},
   "source": [
    "To improve training stability, we normalize the input images with mean and standard deviation values specific to the CIFAR-10 dataset, the `Normalized_ResNet` class extends `ResNet` through this normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc535686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalized_ResNet(ResNet):\n",
    "    def __init__(self, device=\"cuda\", depth=18, num_classes=10):\n",
    "        if depth == 18:\n",
    "            super(Normalized_ResNet, self).__init__(BasicBlock, [2,2,2,2], num_classes)\n",
    "        elif depth == 50:\n",
    "            super(Normalized_ResNet, self).__init__(Bottleneck, [3,4,6,3], num_classes)\n",
    "        elif depth == 26:\n",
    "            super(Normalized_ResNet, self).__init__(BasicBlock, [3,3,3,3], num_classes)\n",
    "        else:\n",
    "            pass \n",
    "\n",
    "        self.mu = torch.Tensor([0.4914, 0.4822, 0.4465]).float().view(3, 1, 1).to(device)\n",
    "        self.sigma = torch.Tensor([0.2023, 0.1994, 0.2010]).float().view(3, 1, 1).to(device)\n",
    "\n",
    "    def forward(self, x, feature_maps=False):\n",
    "        x = (x - self.mu) / self.sigma\n",
    "        return super(Normalized_ResNet, self).forward(x, feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17369e5",
   "metadata": {},
   "source": [
    "We begin by initializing our neural network, as well as our data augmentation stack, which involves random cropping and horizontal flipping of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e02aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'saved_models/pretrained/trained_model.pth'\n",
    "net = Normalized_ResNet(depth=26)\n",
    "net.to(device)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500773c0",
   "metadata": {},
   "source": [
    "We now load the CIFAR-10 dataset, as well as take a subset of size 5000 of the dataset as a proof of concept, creating training and testing sets of 5000 images each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10616f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(\"./data\", True, transform=train_transform, download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(\"./data\", False, transform=transforms.Compose([transforms.ToTensor()]), download=True)\n",
    "subset_size = 5000\n",
    "train_data, _ = random_split(train_data, [subset_size, len(train_data) - subset_size])\n",
    "test_data, _ = random_split(test_data, [subset_size, len(test_data) - subset_size])\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_data, batch_size=256, shuffle=True, num_workers=4)\n",
    "test_loader = torch_data.DataLoader(test_data, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffea86a",
   "metadata": {},
   "source": [
    "We now initialize our optimizer, scheduler, and loss function:\n",
    "- **Optimizer**: We use SGD with momentum, which helps stabilize the training\n",
    "- **Scheduler**: Cosine Annealing is applied to the learning rate, which gradually reduces it to optimize training speed\n",
    "- **Loss Function**: Cross-Entropy Loss is used, which is standard for multi-class classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114f4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "ce_loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd4ff7",
   "metadata": {},
   "source": [
    "We now train our model on 200 epochs, and for each epoch we:\n",
    "1. **Train**: Perform forward pass, compute loss, and backpropagate the gradients\n",
    "2. **Update**: Clip gradients to prevent insatbility and then update model parameters\n",
    "3. **Evaluate**: Calculate accuracy on the test set at each epoch and save the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003e4519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 : Acc : 0.06319999694824219\n",
      "Epoch : 1 : Acc : 0.12629999220371246\n",
      "Epoch : 2 : Acc : 0.17409999668598175\n",
      "Epoch : 3 : Acc : 0.14509999752044678\n",
      "Epoch : 4 : Acc : 0.17569999396800995\n",
      "Epoch : 5 : Acc : 0.13989999890327454\n",
      "Epoch : 6 : Acc : 0.1711999922990799\n",
      "Epoch : 7 : Acc : 0.19619999825954437\n",
      "Epoch : 8 : Acc : 0.17109999060630798\n",
      "Epoch : 9 : Acc : 0.19429999589920044\n",
      "Epoch : 10 : Acc : 0.23709999024868011\n",
      "Epoch : 11 : Acc : 0.2556999921798706\n",
      "Epoch : 12 : Acc : 0.2361999899148941\n",
      "Epoch : 13 : Acc : 0.24549999833106995\n",
      "Epoch : 14 : Acc : 0.25849997997283936\n",
      "Epoch : 15 : Acc : 0.25999999046325684\n",
      "Epoch : 16 : Acc : 0.24809999763965607\n",
      "Epoch : 17 : Acc : 0.2662000060081482\n",
      "Epoch : 18 : Acc : 0.2759000062942505\n",
      "Epoch : 19 : Acc : 0.23809999227523804\n",
      "Epoch : 20 : Acc : 0.27799999713897705\n",
      "Epoch : 21 : Acc : 0.24959999322891235\n",
      "Epoch : 22 : Acc : 0.2525999844074249\n",
      "Epoch : 23 : Acc : 0.28360000252723694\n",
      "Epoch : 24 : Acc : 0.2712000012397766\n",
      "Epoch : 25 : Acc : 0.27219998836517334\n",
      "Epoch : 26 : Acc : 0.3174999952316284\n",
      "Epoch : 27 : Acc : 0.23269999027252197\n",
      "Epoch : 28 : Acc : 0.30949997901916504\n",
      "Epoch : 29 : Acc : 0.29989999532699585\n",
      "Epoch : 30 : Acc : 0.3089999854564667\n",
      "Epoch : 31 : Acc : 0.2929999828338623\n",
      "Epoch : 32 : Acc : 0.282399982213974\n",
      "Epoch : 33 : Acc : 0.27889999747276306\n",
      "Epoch : 34 : Acc : 0.28459998965263367\n",
      "Epoch : 35 : Acc : 0.3264999985694885\n",
      "Epoch : 36 : Acc : 0.3321000039577484\n",
      "Epoch : 37 : Acc : 0.32690000534057617\n",
      "Epoch : 38 : Acc : 0.32170000672340393\n",
      "Epoch : 39 : Acc : 0.353300005197525\n",
      "Epoch : 40 : Acc : 0.3130999803543091\n",
      "Epoch : 41 : Acc : 0.33219999074935913\n",
      "Epoch : 42 : Acc : 0.3278999924659729\n",
      "Epoch : 43 : Acc : 0.3490999937057495\n",
      "Epoch : 44 : Acc : 0.3111000061035156\n",
      "Epoch : 45 : Acc : 0.35569998621940613\n",
      "Epoch : 46 : Acc : 0.30379998683929443\n",
      "Epoch : 47 : Acc : 0.3147999942302704\n",
      "Epoch : 48 : Acc : 0.3335999846458435\n",
      "Epoch : 49 : Acc : 0.34139999747276306\n",
      "Epoch : 50 : Acc : 0.3391000032424927\n",
      "Epoch : 51 : Acc : 0.34039998054504395\n",
      "Epoch : 52 : Acc : 0.34450000524520874\n",
      "Epoch : 53 : Acc : 0.33319997787475586\n",
      "Epoch : 54 : Acc : 0.3643999993801117\n",
      "Epoch : 55 : Acc : 0.33570000529289246\n",
      "Epoch : 56 : Acc : 0.3422999978065491\n",
      "Epoch : 57 : Acc : 0.33660000562667847\n",
      "Epoch : 58 : Acc : 0.32749998569488525\n",
      "Epoch : 59 : Acc : 0.33399999141693115\n",
      "Epoch : 60 : Acc : 0.34379997849464417\n",
      "Epoch : 61 : Acc : 0.36419999599456787\n",
      "Epoch : 62 : Acc : 0.3498999774456024\n",
      "Epoch : 63 : Acc : 0.3549000024795532\n",
      "Epoch : 64 : Acc : 0.3577999770641327\n",
      "Epoch : 65 : Acc : 0.34139999747276306\n",
      "Epoch : 66 : Acc : 0.3409999907016754\n",
      "Epoch : 67 : Acc : 0.3375999927520752\n",
      "Epoch : 68 : Acc : 0.34849998354911804\n",
      "Epoch : 69 : Acc : 0.32899999618530273\n",
      "Epoch : 70 : Acc : 0.35899999737739563\n",
      "Epoch : 71 : Acc : 0.35189998149871826\n",
      "Epoch : 72 : Acc : 0.33159998059272766\n",
      "Epoch : 73 : Acc : 0.35850000381469727\n",
      "Epoch : 74 : Acc : 0.3465999960899353\n",
      "Epoch : 75 : Acc : 0.3671000003814697\n",
      "Epoch : 76 : Acc : 0.37389999628067017\n",
      "Epoch : 77 : Acc : 0.35199999809265137\n",
      "Epoch : 78 : Acc : 0.3357999920845032\n",
      "Epoch : 79 : Acc : 0.36149999499320984\n",
      "Epoch : 80 : Acc : 0.35099998116493225\n",
      "Epoch : 81 : Acc : 0.3474999964237213\n",
      "Epoch : 82 : Acc : 0.3587999939918518\n",
      "Epoch : 83 : Acc : 0.37219998240470886\n",
      "Epoch : 84 : Acc : 0.3563999831676483\n",
      "Epoch : 85 : Acc : 0.34769999980926514\n",
      "Epoch : 86 : Acc : 0.3725999891757965\n",
      "Epoch : 87 : Acc : 0.3633999824523926\n",
      "Epoch : 88 : Acc : 0.35909998416900635\n",
      "Epoch : 89 : Acc : 0.36039999127388\n",
      "Epoch : 90 : Acc : 0.3637000024318695\n",
      "Epoch : 91 : Acc : 0.35929998755455017\n",
      "Epoch : 92 : Acc : 0.3617999851703644\n",
      "Epoch : 93 : Acc : 0.3553999960422516\n",
      "Epoch : 94 : Acc : 0.35989999771118164\n",
      "Epoch : 95 : Acc : 0.35169997811317444\n",
      "Epoch : 96 : Acc : 0.3734999895095825\n",
      "Epoch : 97 : Acc : 0.36059999465942383\n",
      "Epoch : 98 : Acc : 0.3667999804019928\n",
      "Epoch : 99 : Acc : 0.3531999886035919\n",
      "Epoch : 100 : Acc : 0.36649999022483826\n",
      "Epoch : 101 : Acc : 0.37299999594688416\n",
      "Epoch : 102 : Acc : 0.34599998593330383\n",
      "Epoch : 103 : Acc : 0.3772999942302704\n",
      "Epoch : 104 : Acc : 0.36910000443458557\n",
      "Epoch : 105 : Acc : 0.3725000023841858\n",
      "Epoch : 106 : Acc : 0.374099999666214\n",
      "Epoch : 107 : Acc : 0.3771999776363373\n",
      "Epoch : 108 : Acc : 0.3686999976634979\n",
      "Epoch : 109 : Acc : 0.36249998211860657\n",
      "Epoch : 110 : Acc : 0.3637000024318695\n",
      "Epoch : 111 : Acc : 0.37709999084472656\n",
      "Epoch : 112 : Acc : 0.37199997901916504\n",
      "Epoch : 113 : Acc : 0.3772999942302704\n",
      "Epoch : 114 : Acc : 0.3726999759674072\n",
      "Epoch : 115 : Acc : 0.37599998712539673\n",
      "Epoch : 116 : Acc : 0.3763999938964844\n",
      "Epoch : 117 : Acc : 0.3709999918937683\n",
      "Epoch : 118 : Acc : 0.3806999921798706\n",
      "Epoch : 119 : Acc : 0.38429999351501465\n",
      "Epoch : 120 : Acc : 0.37279999256134033\n",
      "Epoch : 121 : Acc : 0.37529999017715454\n",
      "Epoch : 122 : Acc : 0.3799999952316284\n",
      "Epoch : 123 : Acc : 0.3845999836921692\n",
      "Epoch : 124 : Acc : 0.38599997758865356\n",
      "Epoch : 125 : Acc : 0.3798999786376953\n",
      "Epoch : 126 : Acc : 0.3788999915122986\n",
      "Epoch : 127 : Acc : 0.37700000405311584\n",
      "Epoch : 128 : Acc : 0.38329997658729553\n",
      "Epoch : 129 : Acc : 0.38019999861717224\n",
      "Epoch : 130 : Acc : 0.3779999911785126\n",
      "Epoch : 131 : Acc : 0.3852999806404114\n",
      "Epoch : 132 : Acc : 0.38359999656677246\n",
      "Epoch : 133 : Acc : 0.3805999755859375\n",
      "Epoch : 134 : Acc : 0.3861999809741974\n",
      "Epoch : 135 : Acc : 0.38609999418258667\n",
      "Epoch : 136 : Acc : 0.38279998302459717\n",
      "Epoch : 137 : Acc : 0.38199999928474426\n",
      "Epoch : 138 : Acc : 0.3763999938964844\n",
      "Epoch : 139 : Acc : 0.38830000162124634\n",
      "Epoch : 140 : Acc : 0.3917999863624573\n",
      "Epoch : 141 : Acc : 0.38509997725486755\n",
      "Epoch : 142 : Acc : 0.3865000009536743\n",
      "Epoch : 143 : Acc : 0.38670000433921814\n",
      "Epoch : 144 : Acc : 0.38329997658729553\n",
      "Epoch : 145 : Acc : 0.3917999863624573\n",
      "Epoch : 146 : Acc : 0.38179999589920044\n",
      "Epoch : 147 : Acc : 0.3814999759197235\n",
      "Epoch : 148 : Acc : 0.3865000009536743\n",
      "Epoch : 149 : Acc : 0.38909998536109924\n",
      "Epoch : 150 : Acc : 0.3911999762058258\n",
      "Epoch : 151 : Acc : 0.3928999900817871\n",
      "Epoch : 152 : Acc : 0.3879999816417694\n",
      "Epoch : 153 : Acc : 0.3926999866962433\n",
      "Epoch : 154 : Acc : 0.39169999957084656\n",
      "Epoch : 155 : Acc : 0.39100000262260437\n",
      "Epoch : 156 : Acc : 0.3935000002384186\n",
      "Epoch : 157 : Acc : 0.39069998264312744\n",
      "Epoch : 158 : Acc : 0.3933999836444855\n",
      "Epoch : 159 : Acc : 0.3937000036239624\n",
      "Epoch : 160 : Acc : 0.39559999108314514\n",
      "Epoch : 161 : Acc : 0.39159998297691345\n",
      "Epoch : 162 : Acc : 0.3942999839782715\n",
      "Epoch : 163 : Acc : 0.39489999413490295\n",
      "Epoch : 164 : Acc : 0.39469999074935913\n",
      "Epoch : 165 : Acc : 0.39579999446868896\n",
      "Epoch : 166 : Acc : 0.39479997754096985\n",
      "Epoch : 167 : Acc : 0.3937000036239624\n",
      "Epoch : 168 : Acc : 0.3933999836444855\n",
      "Epoch : 169 : Acc : 0.3942999839782715\n",
      "Epoch : 170 : Acc : 0.3950999975204468\n",
      "Epoch : 171 : Acc : 0.39329999685287476\n",
      "Epoch : 172 : Acc : 0.3953000009059906\n",
      "Epoch : 173 : Acc : 0.39489999413490295\n",
      "Epoch : 174 : Acc : 0.3958999812602997\n",
      "Epoch : 175 : Acc : 0.3958999812602997\n",
      "Epoch : 176 : Acc : 0.3944000005722046\n",
      "Epoch : 177 : Acc : 0.39419999718666077\n",
      "Epoch : 178 : Acc : 0.39309999346733093\n",
      "Epoch : 179 : Acc : 0.39479997754096985\n",
      "Epoch : 180 : Acc : 0.3946000039577484\n",
      "Epoch : 181 : Acc : 0.3946000039577484\n",
      "Epoch : 182 : Acc : 0.39640000462532043\n",
      "Epoch : 183 : Acc : 0.3951999843120575\n",
      "Epoch : 184 : Acc : 0.39579999446868896\n",
      "Epoch : 185 : Acc : 0.3944000005722046\n",
      "Epoch : 186 : Acc : 0.39469999074935913\n",
      "Epoch : 187 : Acc : 0.39389997720718384\n",
      "Epoch : 188 : Acc : 0.39419999718666077\n",
      "Epoch : 189 : Acc : 0.3955000042915344\n",
      "Epoch : 190 : Acc : 0.39409998059272766\n",
      "Epoch : 191 : Acc : 0.39419999718666077\n",
      "Epoch : 192 : Acc : 0.39469999074935913\n",
      "Epoch : 193 : Acc : 0.39579999446868896\n",
      "Epoch : 194 : Acc : 0.3953999876976013\n",
      "Epoch : 195 : Acc : 0.3959999978542328\n",
      "Epoch : 196 : Acc : 0.3951999843120575\n",
      "Epoch : 197 : Acc : 0.39499998092651367\n",
      "Epoch : 198 : Acc : 0.39489999413490295\n",
      "Epoch : 199 : Acc : 0.3950999975204468\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "\n",
    "train_loss_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    net.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        preds = net(data)\n",
    "        loss = ce_loss(preds, labels).mean()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1, norm_type=2.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    train_loss_history.append(avg_epoch_loss)    \n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    acc = 0.0\n",
    "    net.eval()\n",
    "    for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        preds = net(data)\n",
    "        acc += (preds.max(1)[1] == labels).float().sum()\n",
    "\n",
    "    acc = acc / 10000\n",
    "    test_acc_history.append(acc.item())\n",
    "    \n",
    "    print(f\"Epoch : {epoch} : Acc : {acc}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc \n",
    "        torch.save({\"net\": net.state_dict()}, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de9a26",
   "metadata": {},
   "source": [
    "We then save our test accuracies and previous training loss values to files to track our progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57da5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('progress/test_acc_history.txt', 'w') as f:\n",
    "    for number in test_acc_history:\n",
    "        f.write(f'{number}\\n')\n",
    "with open('progress/train_loss_history.txt', 'w') as f:\n",
    "    for number in train_loss_history:\n",
    "        f.write(f'{number}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fa8bb",
   "metadata": {},
   "source": [
    "Now that we have trained our source classifier, we can benchmark it at test time using the CIFAR-10C dataset, which contains corrupted versions of the CIFAR-10 dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2c895",
   "metadata": {},
   "source": [
    "We first begin by initializing our neural net and loading the previously saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de1e6335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalized_ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Normalized_ResNet(depth=26)\n",
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.to(device)\n",
    "net.eval()\n",
    "\n",
    "# # Remove 'module.' prefix if checkpoint was saved with DataParallel\n",
    "# checkpoint = {k.replace(\"module.\", \"\"): v for k, v in checkpoint.items()}\n",
    "\n",
    "# net.to(device)\n",
    "# net.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e600d5b",
   "metadata": {},
   "source": [
    "We then define the subset size of CIFAR-10C that we would like to use, as well as lists of severity levels as well as corruption types that we would like to test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1badb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 5000\n",
    "severity_list = [5, 4, 3, 2, 1]\n",
    "corruption_type_list = [\n",
    "    \"gaussian_noise\",\n",
    "    \"shot_noise\",\n",
    "    \"impulse_noise\",\n",
    "    \"defocus_blur\",\n",
    "    \"glass_blur\",\n",
    "    \"motion_blur\",\n",
    "    \"zoom_blur\",\n",
    "    \"snow\",\n",
    "    \"frost\",\n",
    "    \"fog\",\n",
    "    \"brightness\",\n",
    "    \"contrast\",\n",
    "    \"elastic_transform\",\n",
    "    \"pixelate\",\n",
    "    \"jpeg_compression\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f145cd",
   "metadata": {},
   "source": [
    "We now test our classifier, evaluating its ability to generate pseudolabels for various severities of corruptions in CIFAR-10C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eca1d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta test begin!\n",
      "error % [gaussian_noise5]: 88.86%\n",
      "Meta test begin!\n",
      "error % [shot_noise5]: 89.00%\n",
      "Meta test begin!\n",
      "error % [impulse_noise5]: 89.48%\n",
      "Meta test begin!\n",
      "error % [defocus_blur5]: 88.98%\n",
      "Meta test begin!\n",
      "error % [glass_blur5]: 89.04%\n",
      "Meta test begin!\n",
      "error % [motion_blur5]: 88.90%\n",
      "Meta test begin!\n",
      "error % [zoom_blur5]: 89.12%\n",
      "Meta test begin!\n",
      "error % [snow5]: 89.70%\n",
      "Meta test begin!\n",
      "error % [frost5]: 89.76%\n",
      "Meta test begin!\n",
      "error % [fog5]: 89.76%\n",
      "Meta test begin!\n",
      "error % [brightness5]: 89.38%\n",
      "Meta test begin!\n",
      "error % [contrast5]: 88.52%\n",
      "Meta test begin!\n",
      "error % [elastic_transform5]: 89.26%\n",
      "Meta test begin!\n",
      "error % [pixelate5]: 88.66%\n",
      "Meta test begin!\n",
      "error % [jpeg_compression5]: 89.12%\n",
      "Meta test begin!\n",
      "error % [gaussian_noise4]: 89.04%\n",
      "Meta test begin!\n",
      "error % [shot_noise4]: 88.88%\n",
      "Meta test begin!\n",
      "error % [impulse_noise4]: 89.04%\n",
      "Meta test begin!\n",
      "error % [defocus_blur4]: 88.82%\n",
      "Meta test begin!\n",
      "error % [glass_blur4]: 88.92%\n",
      "Meta test begin!\n",
      "error % [motion_blur4]: 88.98%\n",
      "Meta test begin!\n",
      "error % [zoom_blur4]: 89.02%\n",
      "Meta test begin!\n",
      "error % [snow4]: 89.44%\n",
      "Meta test begin!\n",
      "error % [frost4]: 89.62%\n",
      "Meta test begin!\n",
      "error % [fog4]: 89.72%\n",
      "Meta test begin!\n",
      "error % [brightness4]: 89.48%\n",
      "Meta test begin!\n",
      "error % [contrast4]: 88.74%\n",
      "Meta test begin!\n",
      "error % [elastic_transform4]: 88.94%\n",
      "Meta test begin!\n",
      "error % [pixelate4]: 88.84%\n",
      "Meta test begin!\n",
      "error % [jpeg_compression4]: 89.02%\n",
      "Meta test begin!\n",
      "error % [gaussian_noise3]: 89.08%\n",
      "Meta test begin!\n",
      "error % [shot_noise3]: 88.78%\n",
      "Meta test begin!\n",
      "error % [impulse_noise3]: 89.02%\n",
      "Meta test begin!\n",
      "error % [defocus_blur3]: 88.88%\n",
      "Meta test begin!\n",
      "error % [glass_blur3]: 89.12%\n",
      "Meta test begin!\n",
      "error % [motion_blur3]: 89.14%\n",
      "Meta test begin!\n",
      "error % [zoom_blur3]: 89.00%\n",
      "Meta test begin!\n",
      "error % [snow3]: 89.34%\n",
      "Meta test begin!\n",
      "error % [frost3]: 89.60%\n",
      "Meta test begin!\n",
      "error % [fog3]: 89.56%\n",
      "Meta test begin!\n",
      "error % [brightness3]: 89.54%\n",
      "Meta test begin!\n",
      "error % [contrast3]: 88.96%\n",
      "Meta test begin!\n",
      "error % [elastic_transform3]: 89.16%\n",
      "Meta test begin!\n",
      "error % [pixelate3]: 88.98%\n",
      "Meta test begin!\n",
      "error % [jpeg_compression3]: 88.86%\n",
      "Meta test begin!\n",
      "error % [gaussian_noise2]: 88.90%\n",
      "Meta test begin!\n",
      "error % [shot_noise2]: 88.88%\n",
      "Meta test begin!\n",
      "error % [impulse_noise2]: 88.92%\n",
      "Meta test begin!\n",
      "error % [defocus_blur2]: 88.84%\n",
      "Meta test begin!\n",
      "error % [glass_blur2]: 88.86%\n",
      "Meta test begin!\n",
      "error % [motion_blur2]: 88.94%\n",
      "Meta test begin!\n",
      "error % [zoom_blur2]: 89.10%\n",
      "Meta test begin!\n",
      "error % [snow2]: 89.38%\n",
      "Meta test begin!\n",
      "error % [frost2]: 89.44%\n",
      "Meta test begin!\n",
      "error % [fog2]: 89.62%\n",
      "Meta test begin!\n",
      "error % [brightness2]: 89.36%\n",
      "Meta test begin!\n",
      "error % [contrast2]: 89.34%\n",
      "Meta test begin!\n",
      "error % [elastic_transform2]: 88.88%\n",
      "Meta test begin!\n",
      "error % [pixelate2]: 88.86%\n",
      "Meta test begin!\n",
      "error % [jpeg_compression2]: 88.82%\n",
      "Meta test begin!\n",
      "error % [gaussian_noise1]: 88.92%\n",
      "Meta test begin!\n",
      "error % [shot_noise1]: 88.92%\n",
      "Meta test begin!\n",
      "error % [impulse_noise1]: 88.82%\n",
      "Meta test begin!\n",
      "error % [defocus_blur1]: 88.84%\n",
      "Meta test begin!\n",
      "error % [glass_blur1]: 89.10%\n",
      "Meta test begin!\n",
      "error % [motion_blur1]: 88.96%\n",
      "Meta test begin!\n",
      "error % [zoom_blur1]: 89.18%\n",
      "Meta test begin!\n",
      "error % [snow1]: 89.12%\n",
      "Meta test begin!\n",
      "error % [frost1]: 89.50%\n",
      "Meta test begin!\n",
      "error % [fog1]: 89.34%\n",
      "Meta test begin!\n",
      "error % [brightness1]: 89.18%\n",
      "Meta test begin!\n",
      "error % [contrast1]: 89.40%\n",
      "Meta test begin!\n",
      "error % [elastic_transform1]: 89.16%\n",
      "Meta test begin!\n",
      "error % [pixelate1]: 89.06%\n",
      "Meta test begin!\n",
      "error % [jpeg_compression1]: 88.86%\n"
     ]
    }
   ],
   "source": [
    "error_dict = defaultdict(dict)\n",
    "\n",
    "# Define a subset size\n",
    "subset_size = 5000\n",
    "\n",
    "for i, severity in enumerate(severity_list):  # First severity level only for quick test\n",
    "    for j, corruption_type in enumerate(corruption_type_list):  # First corruption type only\n",
    "        # Load a limited subset of the CIFAR-10-C data directly\n",
    "        x_test, y_test = load_cifar10c(subset_size, severity, './data', True, [corruption_type])\n",
    "        num_classes = 10\n",
    "\n",
    "        # Wrap data in TensorDataset\n",
    "        test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "        # Create DataLoader directly with the subset\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=False)\n",
    "\n",
    "        print(\"Meta test begin!\")\n",
    "        net_test = copy.deepcopy(net)\n",
    "\n",
    "        # Use the test DataLoader with the subset\n",
    "        acc = 0.0\n",
    "        net_test.eval()\n",
    "        for x_curr, y_curr in test_loader:\n",
    "            x_curr, y_curr = x_curr.to(device), y_curr.to(device)\n",
    "            outputs = net_test(x_curr)\n",
    "            acc += (outputs.max(1)[1] == y_curr).float().sum()\n",
    "\n",
    "        acc /= subset_size\n",
    "        err = 1. - acc\n",
    "        print(f\"error % [{corruption_type}{severity}]: {err:.2%}\")\n",
    "        \n",
    "        error_dict[corruption_type][severity] = err.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e23c58",
   "metadata": {},
   "source": [
    "We then write our error rates to an external json, which can be used for other data visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1831c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"progress/error_rates.json\", \"w\") as f:\n",
    "    json.dump(error_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f6ad84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
